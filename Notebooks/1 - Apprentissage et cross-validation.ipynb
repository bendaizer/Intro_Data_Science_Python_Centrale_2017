{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Le machine learning représente l'outil de base du Data Scientist. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien que le machine learning soit généralement utilisé comme une boite noire (on n'invente pas de nouveaux algorithmes tous les jours), il est utile de se familiariser avec le fonctionnement de différents algorithmes pour développer un savoir-faire sur la pertinence de tel ou tel algorithme pour un cas donné.\n",
    "\n",
    "Nous allons voir dans ce notebook comment se construit un modèle de machine learning, en commençant par considérer le modèle le plus simple possible : une régression linéaire pour de l'apprentissage supervisé, afin de ne pas s'encombrer de détails mathématiques complexes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du modèle linéaire <a id='model'></a> \n",
    "\n",
    "$$y \\sim f_{\\theta}(x)$$\n",
    "\n",
    "avec comme fonction f une forme linéaire\n",
    "\n",
    "$$f_{\\theta}(x) = \\theta_0 + \\theta_1 x +\\epsilon $$\n",
    "\n",
    "\n",
    "### Hypothèses du modèle linéaire statistique\n",
    "\n",
    "* les erreurs suivent une loi normale de moyenne nulle\n",
    "* la variance est la même pour tous (homoscédasticité) : la variance est la même pour l'ensemble des termes d'erreures gaussiens\n",
    "$Var(\\epsilon_i)=\\sigma$\n",
    "* les termes d'erreurs pour les différents $x_i$ sont indépendants les uns des autres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(x, params=(0,1), var=1):\n",
    "    \"\"\"Generate a linear function f(x)=a*x+b+N(0,1)\n",
    "    \n",
    "    Args:\n",
    "        x (numpy.array()) : vector used to generate the output\n",
    "        params (tuple of size 2) : b=params[0] and a=params[1]\n",
    "        var (int) : noise spread \n",
    "    \n",
    "    Returns:\n",
    "        numpy.array()\n",
    "    \"\"\"\n",
    "    a, b = params[1], params[0]\n",
    "    return b + a*x + var*np.random.normal(size=len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = 10*np.random.random(50)\n",
    "x.sort()\n",
    "my_params = (0,10)\n",
    "y = linear(x, my_params, var = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode des moindres carrés\n",
    "\n",
    "Afin de fixer les coefficients, nous définissons une mesure qui nous permet de connaître de façon empirique la qualité de l'ajustement.\n",
    "\n",
    "\n",
    "Quelques constats :\n",
    "1. Aucune droite ne peut passer par tous les points, vu qu'ils ne sont pas alignés\n",
    "1. Chaque droite passe plus près de certains que d'autres\n",
    "1. Nous décrétons pour le moment que la meilleure droite est celle qui passe le plus près de l'ensemble des points\n",
    "\n",
    "Pour chaque x, nous allons calculer la distance cumulée entre $y_i$ projeté sur la droite à l'abscisse $x_i$, et minimiser cette quantité\n",
    "\n",
    "$$Erreur(\\theta) = \\frac{1}{N}\\sum_i (y_i-f_{\\theta}(x_i))^2$$\n",
    "\n",
    "* Les erreurs s'ajoutent toujours\n",
    "* Pénalité sur les déviations les plus larges\n",
    "* La fonction carré est dérivable\n",
    "\n",
    "Nous souhaitons trouver les paramètres $\\hat \\theta$ tq $Erreur(\\theta) \\ge Erreur(\\hat \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commençons par définir quelques fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_hypothesis(x, params=(0,1)):\n",
    "    \"\"\"Linear function f(x)=a*x+b\n",
    "    \n",
    "    Args:\n",
    "        x (numpy.array()) : vector used to generate the output\n",
    "        params (tuple of size 2) : b=params[0] and a=params[1]\n",
    "    \n",
    "    Returns:\n",
    "        numpy.array()\n",
    "    \"\"\"\n",
    "    a, b = params[1], params[0]\n",
    "    return b + a*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(x, y, func, func_args=None):\n",
    "    \"\"\"Cost function associated with (x,y) couple and function to fit\n",
    "    \n",
    "    Args:\n",
    "        x : feature vector \n",
    "        y : explained variable\n",
    "        func : target function\n",
    "        func_args : arguments of function\n",
    "        \n",
    "    Returns:\n",
    "        scalar value of cost\n",
    "    \"\"\"\n",
    "    if func_args:\n",
    "        return 1./len(x)*sum(np.square(y-func(x,func_args)))\n",
    "    else :\n",
    "        return 1./len(x)*sum(np.square(y-func(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost(x,y,linear_hypothesis, (5,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost(x,y,linear_hypothesis, (0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recherche exhaustive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def params_search():\n",
    "    \"\"\"2d scan for possible values and return best ones\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    params = []\n",
    "    for p0 in np.linspace(0,10):\n",
    "        for p1 in np.linspace(0,10):\n",
    "            p = (p0,p1)\n",
    "            c = cost(x, y, linear_hypothesis, p)\n",
    "            errors.append(c)\n",
    "            params.append(p)\n",
    "    return params[errors.index(min(errors))]\n",
    "\n",
    "best = params_search()\n",
    "print(\"Paramètres optimisant les moindres carrés :\", best)\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x,best[0]+best[1]*x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recherche aléatoire de paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_params_search():\n",
    "    errors = []\n",
    "    params = []\n",
    "    #choix arbitraire de recherche entre 0 et 10\n",
    "    for p in 10*np.random.rand(100,2):\n",
    "        p = tuple(p) \n",
    "        c = cost(x,y, linear, p)\n",
    "        errors.append(c)\n",
    "        params.append(p)\n",
    "    return params[errors.index(min(errors))]\n",
    "\n",
    "best = random_params_search()\n",
    "print(\"Paramètres optimisant les moindres carrés :\", best)\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x,best[0]+best[1]*x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut chercher longtemps, surtout si l'on n'a pas une bonne idée des valeurs des paramètres que l'on cherche.\n",
    "\n",
    "Les problèmes de recherche d'optimum et de racines de fonction sont particulièrement courants. De nombreuses méthodes existent, qui dépassent le cadre de ce cours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descente de gradient\n",
    "\n",
    "On démarre avec des paramètres $\\theta_i$ aléatoires.\n",
    "\n",
    "Plutôt que de tester de nouvelles combinaisons au hasard, nous allons mettre à jour les valeurs que nous avons en ajoutant ou soustrayant une certaine quantité dans la direction qui minimise l'erreur.\n",
    "\n",
    "$\\theta_{i+1} = \\theta_i + \\delta$\n",
    "\n",
    "Si `Erreur` croît, alors sa dérivée est positive, et si elle décroit, alors elle est négative. Nous voulons donc aller dans la direction des dérivées décroissantes.\n",
    "\n",
    "$$\\delta \\propto -\\frac{\\partial E}{\\partial \\theta_i}$$\n",
    "<img src=\"img/2000px-Gradient_descent.svg.png\" width=\"500\">\n",
    "\n",
    "Le gradient vient de la forme vectorielle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, start_params, alpha=0.001, iterations=1000):\n",
    "    params = list(start_params)\n",
    "    N=len(y)\n",
    "    error_history = np.array([]).reshape(0,3)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        params[0] = params[0] - (alpha/N)*sum(linear_hypothesis(x, params) - y)\n",
    "        params[1] = params[1] - (alpha/N)*(linear_hypothesis(x, params) - y).dot(x)\n",
    "        error_history = np.vstack([error_history,\n",
    "                                   [cost(x,y, linear, tuple(params)), \n",
    "                                    params[0], params[1]]])\n",
    "    return params, error_history    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p, error_history = gradient_descent(x, y, (5,2), alpha=0.01, iterations=1000)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifions maintenant notre régression, l'évolution de l'erreur ainsi que des paramètres pendant la descente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(x,y)\n",
    "plt.plot(x, p[0]+p[1]*x, 'g')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(50), error_history[:50,0])\n",
    "plt.title(\"Erreur\")\n",
    "plt.xlabel(\"itération\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(error_history[:,1], label='b')\n",
    "plt.plot(error_history[:,2], label='a')\n",
    "\n",
    "plt.xlabel(\"itération\")\n",
    "plt.ylabel(\"valeur\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour aller plus loin avec l'utilisation par descente de gradient, qui joue un rôle centrale aujourd'hui dans l'optimisation des réseaux de neurones profonds : http://ruder.io/optimizing-gradient-descent/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression avec sklearn et cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons faire la même chose, mais avec la librairie sklearn. Il faut cependant transformer le vecteur X en entrée, en passant d'une liste de nombres à une liste de feature vectors (liste de liste), même si il n'y a qu'un seul élément dans notre feature vector.\n",
    "\n",
    "```\n",
    "[[],\n",
    " [],\n",
    " [],\n",
    " ...\n",
    " []]\n",
    "```\n",
    "\n",
    "Il est en effet plutôt rare de n'avoir qu'un seul élément en entrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_sk = x.reshape(-1,1)\n",
    "print(x_sk[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d'illustrer l'intérêt de la cross-validation et le phénomène d'overfitting, on va utiliser un modèle plus complexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest.fit(x_sk, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(x,y)\n",
    "plt.plot(x, forest.predict(x_sk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost(x,y, linear_hypothesis, (p[0], p[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost(x_sk,y, forest.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La random forest bat à plate couture le modèle linéaire. Mais nous allons maintenant tester le pouvoir prédictif, et la capacité à généraliser à des données qui n'ont pas été utilisées pour l'apprentissage. Pour cela nous allons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4)\n",
    "x_train_sk = x_train.reshape(-1, 1)\n",
    "x_test_sk = x_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest.fit(x_train_sk, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(x_train, y_train)\n",
    "plt.scatter(x_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost(x_train_sk, y_train, forest.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost(x_test_sk, y_test, forest.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost(x_test, y_test, linear_hypothesis, (p[0], p[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost(x_train, y_train, linear_hypothesis, (p[0], p[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'erreur entre le train et le test est plus importante dans le cas de la random forest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
